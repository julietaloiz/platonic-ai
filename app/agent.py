"""
SocraticAgent: A reusable Socratic tutor agent using LangChain and OpenAI.

This agent only responds with open-ended questions to help users think deeper, never giving direct answers or opinions.
"""

import os
import logging
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.messages import trim_messages


# Import the system prompt from the local prompts module
from app.prompts import SYSTEM_PROMPT

# Set up a basic logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SocraticAgent:
    """
    A simple, reusable Socratic agent for philosophical or abstract dialogue.

    Attributes:
        model_name (str): The name of the OpenAI model to use.
        session_id (str): Unique identifier for the conversation session.
        messages (list): Stores the conversation as LangChain message objects.
        max_tokens (int): Maximum number of tokens to keep in context.
    """

    def __init__(self, model_name="gpt-4o-mini", session_id="default", max_tokens=1024):
        # Load API key
        load_dotenv()
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OpenAI API key not found. Please check your .env file.")

        # Set up the LLM and Socratic prompt
        self.model = ChatOpenAI(model=model_name)
        self.prompt = ChatPromptTemplate.from_messages(
            [
                ("system", SYSTEM_PROMPT),
                MessagesPlaceholder(variable_name="messages"),
            ]
        )
        self.chain = self.prompt | self.model
        self.session_id = session_id
        self.max_tokens = max_tokens

        # Use in-memory message history for this session
        def get_session_history(session_id: str):
            return InMemoryChatMessageHistory()

        self.with_message_history = RunnableWithMessageHistory(
            self.chain,
            get_session_history,
            input_messages_key="messages",
        )
        self.messages = []  # Stores the conversation as LangChain message objects

        # Set up a trimmer to limit context size
        self.trimmer = trim_messages(
            max_tokens=self.max_tokens,
            strategy="last",
            token_counter=self.model,
            include_system=True,
            allow_partial=False,
            start_on="human",
        )

    def _trimmed_history(self):
        """
        Return the trimmed message history to fit within the context window.
        """
        return self.trimmer.invoke(self.messages)

    def ask(self, user_input: str) -> str:
        """
        Send user input to the agent and get a Socratic question in response.
        Args:
            user_input (str): The user's message or question.
        Returns:
            str: The Socratic question generated by the agent, or an error message if something goes wrong.
        """
        self.messages.append(HumanMessage(content=user_input))
        try:
            trimmed = self._trimmed_history()
            response = self.with_message_history.invoke(
                {"messages": trimmed},
                config={"configurable": {"session_id": self.session_id}},
            )
            socratic_question = response.content
            self.messages.append(AIMessage(content=socratic_question))
            return socratic_question
        except Exception as e:
            logger.error(f"Error in SocraticAgent.ask: {e}")
            return "Sorry, something went wrong and I couldn't generate a question. Please try again in a moment."

    def ask_stream(self, user_input: str):
        """
        Stream the Socratic agent's response token by token as it is generated.
        Args:
            user_input (str): The user's message or question.
        Yields:
            str: The next token or chunk of the Socratic question as it is generated.
        """
        self.messages.append(HumanMessage(content=user_input))
        try:
            trimmed = self._trimmed_history()
            stream = self.with_message_history.stream(
                {"messages": trimmed},
                config={"configurable": {"session_id": self.session_id}},
            )
            full_response = ""
            for chunk in stream:
                # Each chunk is an AIMessage with a .content attribute
                token = chunk.content
                full_response += token
                yield token
            # Add the full AI response to the history
            self.messages.append(AIMessage(content=full_response))
        except Exception as e:
            logger.error(f"Error in SocraticAgent.ask_stream: {e}")
            yield "Sorry, something went wrong and I couldn't generate a question. Please try again in a moment."

    def get_history(self):
        """
        Return the conversation history as a list of (role, content) tuples.
        Returns:
            list: List of (role, content) tuples for each message in the conversation.
        """
        return [
            ("user" if isinstance(m, HumanMessage) else "tutor", m.content)
            for m in self.messages
        ]
